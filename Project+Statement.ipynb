{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "Most Americans are exposed to a daily dose of false or misleading content — hoaxes, conspiracy theories, fabricated reports, click-bait headlines, and even satire. Such content is collectively referred to as “misinformation\". There have been verified instances of misinformation and disinformation spreading on social media inflicting real harm on society, such as dangerous health decisions and stock market manipulations to name a few [[1]](https://arxiv.org/abs/1707.07592). Such events has led world leaders to identify the massive spread of digital misinformation as [a major global risk](http://reports.weforum.org/global-risks-2013/risk-case-1/digital-wildfires-in-a-hyperconnected-world/?doing_wp_cron=1533730169.0472350120544433593750). The impact of online misinformation spreading through the mechanism of malicious bots might be prevented if the activities of such bots are identified quickly and accurately [[1]](https://arxiv.org/abs/1707.07592). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Statement\n",
    "\n",
    "The aim of this project is to evaluate classification models that analyze tweets data using machine learning techniques to classify authors as human or bot. Some of the classifcation models we incorporate in this project include natural language processing techniques.\n",
    "\n",
    "This project is consists of three high-level sections.\n",
    "<ol>\n",
    "<li>Data Collection</li>\n",
    "<li>Data Processing</li>\n",
    "<li>Modeling</li>\n",
    "</ol>\n",
    "\n",
    "In each section, you will find a description of the methodology employed in that section, a complete set of python codes used in that section, issues encountered (if any) and how they were handled, and remarks on noteworthy findings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Website Structure\n",
    "\n",
    "<font color='red'>--> Include afterwards</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plan to compare k-NN classification models of various k-values, Linear Discriminant Analysis and Quadratic Discriminant Analysis models, and we will also explore adding more advanced features as well as look for possible model performance improvement by excluding some of the features used in our baseline model. \n",
    "\n",
    "For our final report, we will aim to find a model that has an AUC score greater than 0.7063 (see baseline model performance evaluation in Chapter 3). \n",
    "\n",
    "\n",
    "Additionally, if we have time to spare, we may try to compare our model predictions with the same type of models produced by commercial machine learning tool providers such as Amazon Web Services. We are curious to see how our self-created models might compare to commercial level models. It would be quite nice to find our logistic regression classification model performing similarly to a logistic regression classification model created on AWS. \n",
    "\n",
    "\n",
    "Our models will be evaluated by using the [Botometer Python API](https://botometer.iuni.iu.edu/#!/)  to benchmark and improve our models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our previous project statement and goals from Milestone 2 are listed in italic font below, with our updated revisions and/or comments typed in bold font following the arrow.\n",
    "\n",
    "1.\tOur dataset will consist of two main parts: a set of tweets by known bots and a set of tweets by known human accounts.\n",
    " Our current dataset assumes a user to be a bot if its Botometer score is greater than 0.5.  In reality, we do not know with 100% certainty whether the users in our dataset are bot or humans. \n",
    "\n",
    "2.\tThe set of tweets by known bots will be acquired as the tweets originated from the users in the list of known twitter bots provided at the following URL. https://gist.github.com/Plazmaz/da7cd6d427718c104a6791d250aab946#file-bots-txt\n",
    " The list of Twitter bots provided at the URL above contained several deleted accounts, and the accuracy of the list was deemed to be questionable. So, instead of choosing users from the URL above, we acquired the tweets of users that recently began following select official government Twitter accounts. The Botometer score for these users are assumed to be reliable for discerning between Twitter bots and humans.\n",
    "\n",
    "3.\tIn the case that this is not feasible due to limited resources, we will collect at least 30,000 bot tweets to use in our data analysis. \n",
    " We were able to acquire 107,158 Tweets initially for our raw dataset. Subsequent data cleaning and processing revealed erroneous data, duplicate data, and insufficient data issues. After removing handling these issues by removing them from our dataset, we had 82,224 Tweets to train our machine learning model on. \n",
    "\n",
    "4.\tThe set of tweets by known human accounts will be collected from the twitter accounts of famous people, and will be collected in equal amount as the set of bot tweets. \n",
    " We were able to collect Twitter accounts of famous people, but to broaden our scope of data, and to ensure that Tweets from bots would be included in our dataset as well, we expanded our search to include Tweets from the followers of famous people. Unfortunately, it did not turn out to be the case that the number of bots and humans are equal in our dataset. However, we will explore the possibility of creating a third category for our response variable, representing Twitter accounts for which the determination of their bot-like or human-like nature is inconclusive.\n",
    "\n",
    "5.\tWe have access to a Python script which can search for keywords in past tweets as well as new tweets in real-time. This script will be our main tool for acquiring the Twitter dataset. \n",
    " We utilized the Tweepy API to create our dataset. We also utilized the Botometer API to generate our response variable. \n",
    "6.\tThe tweets consist of various metadata, some of which will be selected as features in our models.  Our dependent variable will be a categorical variable representing one of three options—a human, a bot, or an inconclusive source—as the originator of a tweet. \n",
    " Features were selected and extracted for our baseline model from the Tweet data. The dependent variable for our baseline model is an indicator variable with only two options—bot or not. We will look into expanded the number of categories for our response variable in our final report.\n",
    "\n",
    "7.\tWe will use the Botometer API which is available at https://github.com/IUNetSci/botometer-python.git to benchmark and improve our models.\n",
    " We have been using the Botometer API and derived our “true” response variable values from the Botometer score.\n",
    "\n",
    "8.\tTherefore, the aim of this project is to create models that use machine learning techniques to analyze tweets data to classify the author as a human, bot, or inconclusive source. \n",
    " The aim of our project still remains to create machine learning models that can take Tweet data as input and classify the author as human or bot. We will explore the inconclusive option as well for our final report.\n",
    "\n",
    "9.\tWe plan to incorporate classification models and natural language processing (topic modeling and sentiment analysis) in our machine learning algorithms. \n",
    " Our baseline model incorporates a logistic regression classification model and is trained on predictors such as sentiment polarity and sentiment subjectivity that were extracted via natural language processing techniques. For our final report, we plan to incorporate additional natural language processing features such as TF-IDF matrices and word vectors created using Word2Vec.\n",
    "\n",
    "10.\tOur models will be evaluated by using the Botometer Python API to benchmark and improve our models.\n",
    " This will continue to be our main method of model evaluation.\n",
    "\n",
    "\n",
    "For our final report, we will aim to find a model that has an AUC score greater than 0.7063 (see baseline model performance evaluation in Chapter 3). We plan to compare k-NN classification models of various k-values, Linear Discriminant Analysis and Quadratic Discriminant Analysis models, and we will also explore adding more advanced features as well as look for possible model performance improvement by excluding some of the features used in our baseline model. \n",
    "Additionally, if we have time to spare, we may try to compare our model predictions with the same type of models produced by commercial machine learning tool providers such as Amazon Web Services. We are curious to see how our self-created models might compare to commercial level models. It would be quite nice to find our logistic regression classification model performing similarly to a logistic regression classification model created on AWS. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
